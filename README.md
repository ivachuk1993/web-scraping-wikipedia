# ğŸ“Š Web Scraping da Wikipedia

## ğŸ‡§ğŸ‡· PortuguÃªs

Projeto de web scraping em Python que coleta dados da Wikipedia e salva em arquivos CSV.

### DescriÃ§Ã£o Estendida

Este projeto demonstra uma abordagem simples e eficaz para web scraping usando a biblioteca `pandas` do Python, aproveitando sua capacidade de ler diretamente tabelas HTML. O objetivo principal Ã© extrair dados estruturados de uma pÃ¡gina da Wikipedia e salvÃ¡-los em arquivos CSV para anÃ¡lises posteriores.

O foco do projeto Ã© a simplicidade e clareza, tornando-o ideal para iniciantes que desejam entender os conceitos bÃ¡sicos de web scraping sem a complexidade de bibliotecas mais avanÃ§adas como BeautifulSoup ou Scrapy.

AlÃ©m disso, o projeto reforÃ§a boas prÃ¡ticas, como consultar o arquivo `robots.txt` do site para garantir uma coleta Ã©tica dos dados.

Com poucas linhas de cÃ³digo, Ã© possÃ­vel obter e limpar um conjunto de dados sobre expectativa de vida em diferentes paÃ­ses, mostrando o potencial do `pandas` para tarefas leves de web scraping.

O repositÃ³rio inclui o script completo, instruÃ§Ãµes para configurar um ambiente virtual Python e as dependÃªncias necessÃ¡rias (`pandas` e `lxml`).


### ğŸ”§ Funcionalidades
- Realiza scraping de dados da Wikipedia usando `pandas` e `lxml`.
- Salva os dados coletados em arquivos `.csv`.

### ğŸš€ Como usar
1. Clone este repositÃ³rio:  
   `git clone https://github.com/ivachuk1993/web-scraping-wikipedia.git`

2. Entre na pasta do projeto:  
   `cd web-scraping-wikipedia`

3. Crie e ative um ambiente virtual:

   **Windows:**  
   `python -m venv venv`  
   `venv\Scripts\activate`

   **Linux/macOS:**  
   `python3 -m venv venv`  
   `source venv/bin/activate`

4. Instale as dependÃªncias:  
   `pip install pandas lxml`

5. Execute o script principal:  
   `python webscraping_wiki.py`

---

## ğŸ‡ºğŸ‡¸ English

Python web scraping project that collects data from Wikipedia and saves it as CSV files.

### Extended Description
This project demonstrates a simple and effective approach to web scraping using Python's pandas library, leveraging its ability to read HTML tables directly. The main goal is to extract structured data from a Wikipedia page and save it into CSV files for further analysis.

The project focuses on simplicity and clarity, making it ideal for beginners who want to understand the basic concepts of web scraping without the complexity of more advanced libraries like BeautifulSoup or Scrapy.

Additionally, the project emphasizes best practices, such as checking the site's robots.txt file to ensure ethical data collection.

With just a few lines of code, it's possible to obtain and clean a dataset about life expectancy in different countries, showcasing the potential of pandas for light web scraping tasks.

The repository includes the complete script, instructions for setting up a Python virtual environment, and the necessary dependencies (pandas and lxml).

### ğŸ”§ Features
- Scrapes data from Wikipedia using `pandas` and `lxml`.
- Saves the scraped data into `.csv` files.

### ğŸš€ How to Use
1. Clone this repository:  
   `git clone https://github.com/ivachuk1993/web-scraping-wikipedia.git`

2. Navigate to the project folder:  
   `cd web-scraping-wikipedia`

3. Create and activate a virtual environment:

   **Windows:**  
   `python -m venv venv`  
   `venv\Scripts\activate`

   **Linux/macOS:**  
   `python3 -m venv venv`  
   `source venv/bin/activate`

4. Install dependencies:  
   `pip install pandas lxml`

5. Run the main script:  
   `python webscraping_wiki.py`

---

## ğŸ‘¤ Autora / Author

**VerÃ´nica Ivachuk**  
ğŸ“§ veronicasivachuk@gmail.com  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/veronica-ivachuk/)
